{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2f5b58f-d912-42a4-a1db-a7a93174baaa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gymnasium\n",
      "  Obtaining dependency information for gymnasium from https://files.pythonhosted.org/packages/a8/4d/3cbfd81ed84db450dbe73a89afcd8bc405273918415649ac6683356afe92/gymnasium-0.29.1-py3-none-any.whl.metadata\n",
      "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\ducan\\anaconda3\\lib\\site-packages (from gymnasium) (1.24.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\ducan\\anaconda3\\lib\\site-packages (from gymnasium) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\ducan\\anaconda3\\lib\\site-packages (from gymnasium) (4.9.0)\n",
      "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
      "  Obtaining dependency information for farama-notifications>=0.0.1 from https://files.pythonhosted.org/packages/05/2c/ffc08c54c05cdce6fbed2aeebc46348dbe180c6d2c541c7af7ba0aa5f5f8/Farama_Notifications-0.0.4-py3-none-any.whl.metadata\n",
      "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
      "Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
      "   ---------------------------------------- 0.0/953.9 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 61.4/953.9 kB 1.7 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 297.0/953.9 kB 3.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 665.6/953.9 kB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  952.3/953.9 kB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 953.9/953.9 kB 5.0 MB/s eta 0:00:00\n",
      "Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
      "Installing collected packages: farama-notifications, gymnasium\n",
      "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e302f781-2057-4601-b64b-01f58ac5df37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70780d78-1ab1-4dfb-9931-82cb0bf7bf84",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b2c7c79e-3bf7-4410-b335-e1ce5dcde942",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ChargingEnv(gym.Env):\n",
    "\n",
    "    def __init__(self, render_mode=None, battery_capacity = 50, alpha = [1,1,1,1,1,1,1,1]):\n",
    "        \n",
    "        super(ChargingEnv, self).__init__()\n",
    "        \n",
    "        self.battery_capacity = battery_capacity  # The capacity of the battery\n",
    "        self.alpha = alpha  # The price coefficients per interval\n",
    "        self.num_intervals = 8 # 15-minute intervals in 2 hours\n",
    "        self.mu = 30 # expected energy demand\n",
    "        self.sigma = 5 # sd of energy demand\n",
    "        \n",
    "        # Action space: 4 different charging rates, 0 (no charge), 1 (low charge), 2 (medium charge), 3 (high charge)\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        \n",
    "        # Observation space: SoC (continuous), Time Step (discrete)\n",
    "        self.observation_space = spaces.Tuple((\n",
    "            spaces.Box(low=0, high=self.battery_capacity, dtype=np.float32),\n",
    "            spaces.Discrete(self.num_intervals)\n",
    "        ))\n",
    "        \n",
    "        \n",
    "        # Initialize SoC for the very first episode\n",
    "        initial_demand = np.random.normal(self.mu, self.sigma)\n",
    "        initial_demand = np.clip(initial_demand, 0, self.battery_capacity)\n",
    "        self.residual_soc = self.battery_capacity - initial_demand # SoC from episode before, assuming it was a full battery minus one working day\n",
    "        self.soc = self.residual_soc\n",
    "        \n",
    "        self.time_step = 0\n",
    "    \n",
    "        # Metrics\n",
    "        self.action_frequency = np.zeros(self.action_space.n)\n",
    "        self.actions_per_time_slot = np.zeros((self.num_intervals, self.action_space.n))\n",
    "        self.total_recharge_cost = 0.0\n",
    "        self.energy_demand = 0.0\n",
    "        self.residual_energy = 0.0\n",
    "        self.energy_added = 0.0\n",
    "        self.missing_energy = 0.0\n",
    "        self.penalty_count = 0\n",
    "        self.reward = 0\n",
    "\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment to the initial state for a new day.\"\"\"\n",
    "        self.soc = self.residual_soc  # Start with the residual SoC from the previous day\n",
    "        self.time_step = 0\n",
    "\n",
    "        # Reset metrics for next episode\n",
    "        self.action_frequency.fill(0)\n",
    "        self.actions_per_time_slot.fill(0)\n",
    "        self.total_recharge_cost = 0.0\n",
    "        self.energy_demand = 0.0\n",
    "        self.residual_energy = 0.0\n",
    "        self.cumu_energy_added = 0.0\n",
    "        self.missing_energy = 0.0\n",
    "        self.penalty_count = 0\n",
    "        self.reward = 0\n",
    "        \n",
    "        return self._get_observation()\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Execute one time step within the environment.\"\"\"\n",
    "        assert self.action_space.contains(action), f\"{action} is an invalid action\"\n",
    "\n",
    "        charging_rates = [0, 7, 15, 22]  # kW\n",
    "        charging_rate = charging_rates[action]\n",
    "\n",
    "        energy_added = min(charging_rate * 0.25, self.battery_capacity - self.soc)  # 15 minutes is 0.25 hours, ensure SoC does not exceed battery capacity\n",
    "        self.cumu_energy_added += energy_added\n",
    "        self.soc = self.soc + energy_added\n",
    "        \n",
    "        cost =  self._calculate_cost(charging_rate)\n",
    "        self.reward -= cost\n",
    "        \n",
    "        done = self.time_step+1 >= self.num_intervals # Check if charging window is over\n",
    "        if done:\n",
    "            self.reward = self._calculate_end_of_day_reward()\n",
    "\n",
    "        # Update metrics\n",
    "        self.action_frequency[action] += 1\n",
    "        self.actions_per_time_slot[self.time_step, action] += 1\n",
    "        self.total_recharge_cost += cost # cost based on charging rate not actual energy charged\n",
    "        \n",
    "        self.time_step += 1\n",
    "        \n",
    "        observation = self._get_observation()\n",
    "        return observation, self.reward, done, {}\n",
    "\n",
    "\n",
    "    def _get_observation(self):\n",
    "        \"\"\"Get the current observation.\"\"\"\n",
    "        return (np.array([self.soc], dtype=np.float32), self.time_step)\n",
    "\n",
    "    def _calculate_cost(self, charging_rate):\n",
    "        \"\"\"Calculate the charging cost based on the exponential function.\"\"\"\n",
    "        alpha_t = self.alpha[self.time_step]  # coefficient for time based price\n",
    "        return alpha_t * np.exp(charging_rate)\n",
    "    \n",
    "    def _calculate_end_of_day_reward(self):\n",
    "        \"\"\"Calculate the reward at the end of the day.\"\"\"\n",
    "        self.energy_demand = np.random.normal(self.mu, self.sigma)\n",
    "        if self.soc < self.energy_demand:\n",
    "            self.missing_energy = self.energy_demand - self.soc\n",
    "            penalty = np.exp(22) * 8 * min(1, self.missing_energy) # penalty for not meeting demand is worse than charging fully for 8 timeslots, proportional to missing demand        \n",
    "            self.reward-= penalty\n",
    "            self.residual_soc = 0  # Not enough charge, set residual SoC to 0\n",
    "            self.penalty_count += 1\n",
    "        else:\n",
    "            self.missing_energy = 0\n",
    "            self.residual_soc = self.soc - self.energy_demand  # Update residual SoC for the next day\n",
    "        \n",
    "        return self.reward\n",
    "\n",
    "    def report_metrics(self):\n",
    "        \"\"\"Report the metrics at the end of an episode.\"\"\"\n",
    "        metrics = {\n",
    "            \"action_frequency\": self.action_frequency,\n",
    "            \"actions_per_time_slot\": self.actions_per_time_slot,\n",
    "            \"energy_demand\": self.energy_demand,\n",
    "            \"residual_energy\": self.residual_soc,\n",
    "            \"energy_added\": self.cumu_energy_added,\n",
    "            \"missing_energy\": self.missing_energy,\n",
    "            \"total_recharge_cost\": self.total_recharge_cost,\n",
    "            \"penalty_count\": self.penalty_count,\n",
    "            \"reward\": self.reward\n",
    "        }\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65778913-03fe-4435-8467-40cf8aac92c0",
   "metadata": {},
   "source": [
    "## Basic Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c48986c-1d18-425d-8af2-7584f380ff74",
   "metadata": {},
   "source": [
    "\n",
    "ep_2_residual_energy = ep_1_residual_energy + ep_2_energy_added - ep_2_energy demand  \n",
    "logic: residual episode 1 energy after charging and demand, now in episode 2 we charge and subtract demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8981e53b-d7ee-413e-8f90-b761943874c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: {'action_frequency': array([0., 0., 8., 0.]), 'actions_per_time_slot': array([[0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.]]), 'energy_demand': 27.833224196732015, 'residual_energy': 15.791470074846256, 'energy_added': 30.0, 'missing_energy': 0, 'total_recharge_cost': 26152138.979776885, 'penalty_count': 0, 'reward': -26152138.979776885}\n",
      "Episode 2: {'action_frequency': array([0., 0., 8., 0.]), 'actions_per_time_slot': array([[0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.]]), 'energy_demand': 25.163856600020388, 'residual_energy': 20.62761347482587, 'energy_added': 30.0, 'missing_energy': 0, 'total_recharge_cost': 26152138.979776885, 'penalty_count': 0, 'reward': -26152138.979776885}\n",
      "Episode 3: {'action_frequency': array([0., 0., 8., 0.]), 'actions_per_time_slot': array([[0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.]]), 'energy_demand': 21.811236983755894, 'residual_energy': 28.188763016244106, 'energy_added': 29.37238652517413, 'missing_energy': 0, 'total_recharge_cost': 26152138.979776885, 'penalty_count': 0, 'reward': -26152138.979776885}\n",
      "Episode 4: {'action_frequency': array([0., 0., 8., 0.]), 'actions_per_time_slot': array([[0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.]]), 'energy_demand': 35.49880962918134, 'residual_energy': 14.501190370818662, 'energy_added': 21.81123698375589, 'missing_energy': 0, 'total_recharge_cost': 26152138.979776885, 'penalty_count': 0, 'reward': -26152138.979776885}\n",
      "Episode 5: {'action_frequency': array([0., 0., 8., 0.]), 'actions_per_time_slot': array([[0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.]]), 'energy_demand': 38.21840810407129, 'residual_energy': 6.282782266747375, 'energy_added': 30.0, 'missing_energy': 0, 'total_recharge_cost': 26152138.979776885, 'penalty_count': 0, 'reward': -26152138.979776885}\n",
      "Episode 6: {'action_frequency': array([0., 0., 8., 0.]), 'actions_per_time_slot': array([[0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.]]), 'energy_demand': 31.620793147226397, 'residual_energy': 4.661989119520978, 'energy_added': 30.0, 'missing_energy': 0, 'total_recharge_cost': 26152138.979776885, 'penalty_count': 0, 'reward': -26152138.979776885}\n",
      "Episode 7: {'action_frequency': array([0., 0., 8., 0.]), 'actions_per_time_slot': array([[0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.]]), 'energy_demand': 33.11353289124784, 'residual_energy': 1.548456228273139, 'energy_added': 30.0, 'missing_energy': 0, 'total_recharge_cost': 26152138.979776885, 'penalty_count': 0, 'reward': -26152138.979776885}\n",
      "Episode 8: {'action_frequency': array([0., 0., 8., 0.]), 'actions_per_time_slot': array([[0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.]]), 'energy_demand': 41.83469252307449, 'residual_energy': 0, 'energy_added': 30.0, 'missing_energy': 10.286236294801348, 'total_recharge_cost': 26152138.979776885, 'penalty_count': 1, 'reward': -28705454908.032513}\n",
      "Episode 9: {'action_frequency': array([0., 0., 8., 0.]), 'actions_per_time_slot': array([[0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.]]), 'energy_demand': 28.302106471309635, 'residual_energy': 1.697893528690365, 'energy_added': 30.0, 'missing_energy': 0, 'total_recharge_cost': 26152138.979776885, 'penalty_count': 0, 'reward': -26152138.979776885}\n",
      "Episode 10: {'action_frequency': array([0., 0., 8., 0.]), 'actions_per_time_slot': array([[0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.]]), 'energy_demand': 29.445748155982017, 'residual_energy': 2.2521453727083482, 'energy_added': 30.0, 'missing_energy': 0, 'total_recharge_cost': 26152138.979776885, 'penalty_count': 0, 'reward': -26152138.979776885}\n"
     ]
    }
   ],
   "source": [
    "env = ChargingEnv()\n",
    "\n",
    "# simple fixed policy\n",
    "def fixed_policy(state):\n",
    "    return 2  # Always choose the medium charging rate\n",
    "\n",
    "num_episodes = 10\n",
    "for episode in range(num_episodes):\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done: # does 8 time steps\n",
    "        action = fixed_policy(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        state = next_state\n",
    "    # After each episode, print the metrics\n",
    "    metrics = env.report_metrics()\n",
    "    print(f\"Episode {episode + 1}: {metrics}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
