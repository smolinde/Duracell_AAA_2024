{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7cf3192-a40e-471a-affa-4eb4127ebbfd",
   "metadata": {},
   "source": [
    "# Machine Learning - Data Preparation\n",
    "\n",
    "This script prepares previously gained datasets for actual machine learning training, validation, and testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be1f1493-7b3e-4e2c-9d8b-967e51e828bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this command to install all required libraries for this script\n",
    "!pip install -q pandas matplotlib torch scikit-learn joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b902ef5-5d62-45f6-9653-1df3eee0faaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ba10ab2-2117-4337-96d4-eaec99ab121f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all dataset for three geospatial resolutions\n",
    "df_merged = [None for _ in range(3)]\n",
    "df_merged[0] = pd.read_pickle('../../Datasets/merged_df_6.pkl')\n",
    "df_merged[1] = pd.read_pickle('../../Datasets/merged_df_7.pkl')\n",
    "df_merged[2] = pd.read_pickle('../../Datasets/merged_df_8.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b4bd691-17e5-49e6-a3be-c88eea66e501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createdataset(df, timebucket):\n",
    "    # Copy the original DataFrame to avoid modifying it directly\n",
    "    df_copy = df.copy()\n",
    "    if timebucket == 1:\n",
    "        # Use 'hour_of_day' as 'time_bucket' if timebucket is 1\n",
    "        df_copy['time_bucket'] = df_copy['hour_of_day']\n",
    "        # Remove 'hour_of_day' and 'date' columns\n",
    "        df_copy.drop(columns=['hour_of_day', 'date'], inplace=True)\n",
    "        return df_copy\n",
    "    # Create time buckets based on 'hour_of_day' and the specified 'timebucket' size\n",
    "    df_copy['time_bucket'] = pd.cut(df_copy['hour_of_day'], bins=range(-1, 25, timebucket), labels=False)\n",
    "    # Drop the original 'hour_of_day' column and convert 'time_bucket' to integer\n",
    "    df_copy.drop(columns=['hour_of_day'], inplace=True)\n",
    "    df_copy['time_bucket'] = df_copy['time_bucket'].astype(int)\n",
    "    # Aggregate the DataFrame by mean for weather-related columns\n",
    "    for col in ['temperature', 'dew_point', 'humidity', \n",
    "                'wind_speed', 'wind_gust', 'pressure', \n",
    "                'precipitation_rate']:\n",
    "        df_copy[col] = df_copy.groupby(['time_bucket', 'date'])[col].transform('mean')\n",
    "    # Sum up the 'demand' column within each group of 'date', 'time_bucket', and 'hex_id'\n",
    "    df_copy['demand'] = df_copy.groupby(['date', 'time_bucket', 'hex_id'])['demand'].transform('sum')\n",
    "    # Remove duplicate rows based on 'hex_id', 'date', and 'time_bucket'\n",
    "    df_copy.drop_duplicates(subset=['hex_id', 'date', 'time_bucket'], inplace=True)\n",
    "    # Drop the 'date' column as it is no longer needed\n",
    "    df_copy.drop(columns=['date'], inplace=True)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f37ce6f3-8ed7-42bd-916c-df9633a84e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "# Define time bucket intervals\n",
    "timebuckets = [1, 2, 4, 6, 24]\n",
    "# Initialize empty lists for data storage\n",
    "num_resolutions = len(df_merged)\n",
    "df_data = [[None for _ in range(len(timebuckets))] for _ in range(num_resolutions)]\n",
    "df_train_X = [[None for _ in range(len(timebuckets))] for _ in range(num_resolutions)]\n",
    "df_valid_X = [[None for _ in range(len(timebuckets))] for _ in range(num_resolutions)]\n",
    "df_test_X = [[None for _ in range(len(timebuckets))] for _ in range(num_resolutions)]\n",
    "df_train_y = [[None for _ in range(len(timebuckets))] for _ in range(num_resolutions)]\n",
    "df_valid_y = [[None for _ in range(len(timebuckets))] for _ in range(num_resolutions)]\n",
    "df_test_y = [[None for _ in range(len(timebuckets))] for _ in range(num_resolutions)]\n",
    "train_dl = [[None for _ in range(len(timebuckets))] for _ in range(num_resolutions)]\n",
    "val_dl = [[None for _ in range(len(timebuckets))] for _ in range(num_resolutions)]\n",
    "test_dl = [[None for _ in range(len(timebuckets))] for _ in range(num_resolutions)]\n",
    "# Iterate over each resolution and time bucket\n",
    "for i in range(num_resolutions):\n",
    "    for j in range(len(timebuckets)):\n",
    "        # Initialize scalers for features and target\n",
    "        feature_scaler = MinMaxScaler()\n",
    "        target_scaler = MinMaxScaler()\n",
    "        # Create dataset for the current resolution and time bucket\n",
    "        df_data[i][j] = createdataset(df_merged[i], timebuckets[j])\n",
    "        # Encode 'hex_id' using LabelEncoder\n",
    "        df_data[i][j]['hex_id'] = label_encoder.fit_transform(df_data[i][j]['hex_id'])\n",
    "        # Separate features (X) and target (y)\n",
    "        X = df_data[i][j].drop('demand', axis=1).values\n",
    "        y = df_data[i][j]['demand'].values\n",
    "        # Split data into training and temp sets, then split temp into validation and test sets\n",
    "        df_train_X[i][j], X_temp, df_train_y[i][j], y_temp = train_test_split(\n",
    "            X, y, test_size=0.3, random_state=42)\n",
    "        df_valid_X[i][j], df_test_X[i][j], df_valid_y[i][j], df_test_y[i][j] = train_test_split(\n",
    "            X_temp, y_temp, test_size=1/3, random_state=42)\n",
    "        # Scale features using MinMaxScaler\n",
    "        df_train_X[i][j] = feature_scaler.fit_transform(df_train_X[i][j])\n",
    "        df_valid_X[i][j] = feature_scaler.transform(df_valid_X[i][j])\n",
    "        df_test_X[i][j] = feature_scaler.transform(df_test_X[i][j])\n",
    "        # Save the feature scaler\n",
    "        joblib.dump(feature_scaler, f'scalers/feature_scaler_res_{i + 6}_bucket_{timebuckets[j]}.pkl')\n",
    "        # Scale target using MinMaxScaler\n",
    "        df_train_y[i][j] = target_scaler.fit_transform(df_train_y[i][j].reshape(-1, 1)).flatten()\n",
    "        df_valid_y[i][j] = target_scaler.transform(df_valid_y[i][j].reshape(-1, 1)).flatten()\n",
    "        df_test_y[i][j] = target_scaler.transform(df_test_y[i][j].reshape(-1, 1)).flatten()\n",
    "        # Save the target scaler\n",
    "        joblib.dump(target_scaler, f'scalers/target_scaler_res_{i + 6}_bucket_{timebuckets[j]}.pkl')\n",
    "        # Convert data to PyTorch tensors\n",
    "        df_train_X[i][j] = torch.tensor(df_train_X[i][j], dtype=torch.float32)\n",
    "        df_train_y[i][j] = torch.tensor(df_train_y[i][j], dtype=torch.float32).view(-1, 1)\n",
    "        df_valid_X[i][j] = torch.tensor(df_valid_X[i][j], dtype=torch.float32)\n",
    "        df_valid_y[i][j] = torch.tensor(df_valid_y[i][j], dtype=torch.float32).view(-1, 1)\n",
    "        df_test_X[i][j] = torch.tensor(df_test_X[i][j], dtype=torch.float32)\n",
    "        df_test_y[i][j] = torch.tensor(df_test_y[i][j], dtype=torch.float32).view(-1, 1)\n",
    "        # Save datasets as PyTorch TensorDataset\n",
    "        torch.save(TensorDataset(df_train_X[i][j], df_train_y[i][j]), f'datasets/train_dataset_res_{i + 6}_bucket_{timebuckets[j]}.pt')\n",
    "        torch.save(TensorDataset(df_valid_X[i][j], df_valid_y[i][j]), f'datasets/valid_dataset_res_{i + 6}_bucket_{timebuckets[j]}.pt')\n",
    "        torch.save(TensorDataset(df_test_X[i][j], df_test_y[i][j]), f'datasets/test_dataset_res_{i + 6}_bucket_{timebuckets[j]}.pt')\n",
    "\n",
    "# Delete variables to free up memory\n",
    "del(df_merged, df_data, df_train_X, df_valid_X, df_test_X, df_train_y, df_valid_y, df_test_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
