{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b902ef5-5d62-45f6-9653-1df3eee0faaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ba10ab2-2117-4337-96d4-eaec99ab121f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = [None for _ in range(3)]\n",
    "df_merged[0] = pd.read_pickle('../../Datasets/merged_df_6.pkl')\n",
    "df_merged[1] = pd.read_pickle('../../Datasets/merged_df_7.pkl')\n",
    "df_merged[2] = pd.read_pickle('../../Datasets/merged_df_8.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b4bd691-17e5-49e6-a3be-c88eea66e501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createdataset(df, timebucket):\n",
    "    if timebucket == 1:\n",
    "        df_copy = df.copy()\n",
    "        df_copy['time_bucket'] = df_copy['hour_of_day']\n",
    "        df_copy.drop(columns=['hour_of_day', 'date'], inplace = True)\n",
    "        return df_copy\n",
    "    df_copy = df.copy()\n",
    "    df_copy['time_bucket'] = pd.cut(df_copy['hour_of_day'], bins=range(-1, 25, timebucket), labels=False)\n",
    "    df_copy = df_copy.drop(columns= ['hour_of_day'])\n",
    "    df_copy['time_bucket'] = df_copy['time_bucket'].astype(int)\n",
    "    df_copy['temperature'] = df_copy.groupby(['time_bucket', 'date'])['temperature'].transform('mean')\n",
    "    df_copy['dew_point'] = df_copy.groupby(['time_bucket', 'date'])['dew_point'].transform('mean')\n",
    "    df_copy['humidity'] = df_copy.groupby(['time_bucket', 'date'])['humidity'].transform('mean')\n",
    "    df_copy['wind_speed'] = df_copy.groupby(['time_bucket', 'date'])['wind_speed'].transform('mean')\n",
    "    df_copy['wind_gust'] = df_copy.groupby(['time_bucket', 'date'])['wind_gust'].transform('mean')\n",
    "    df_copy['pressure'] = df_copy.groupby(['time_bucket', 'date'])['pressure'].transform('mean')\n",
    "    df_copy['precipitation_rate'] = df_copy.groupby(['time_bucket', 'date'])['precipitation_rate'].transform('mean')\n",
    "    df_copy['demand'] = df_copy.groupby(['date', 'time_bucket', 'hex_id'])['demand'].transform('sum')\n",
    "    df_copy = df_copy.drop_duplicates(subset=['hex_id', 'date', 'time_bucket'])\n",
    "    df_copy.drop(columns=['date'], inplace = True)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f37ce6f3-8ed7-42bd-916c-df9633a84e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "timebuckets = [1, 2, 4, 6, 24]\n",
    "num_resolutions = len(df_merged)\n",
    "df_data = [[None for _ in range(len(timebuckets))] for _ in range(num_resolutions)]\n",
    "df_train_X = [[None for _ in range(len(timebuckets))] for _ in range(num_resolutions)]\n",
    "df_valid_X = [[None for _ in range(len(timebuckets))] for _ in range(num_resolutions)]\n",
    "df_test_X = [[None for _ in range(len(timebuckets))] for _ in range(num_resolutions)]\n",
    "df_train_y = [[None for _ in range(len(timebuckets))] for _ in range(num_resolutions)]\n",
    "df_valid_y = [[None for _ in range(len(timebuckets))] for _ in range(num_resolutions)]\n",
    "df_test_y = [[None for _ in range(len(timebuckets))] for _ in range(num_resolutions)]\n",
    "train_dl = [[None for _ in range(len(timebuckets))] for _ in range(num_resolutions)]\n",
    "val_dl = [[None for _ in range(len(timebuckets))] for _ in range(num_resolutions)]\n",
    "test_dl = [[None for _ in range(len(timebuckets))] for _ in range(num_resolutions)]\n",
    "for i in range(num_resolutions):\n",
    "    for j in range(len(timebuckets)):\n",
    "        feature_scaler = MinMaxScaler()\n",
    "        target_scaler = MinMaxScaler()\n",
    "        df_data[i][j] = createdataset(df_merged[i], timebuckets[j])\n",
    "        df_data[i][j]['hex_id'] = label_encoder.fit_transform(df_data[i][j]['hex_id'])\n",
    "        X = df_data[i][j].drop('demand', axis=1).values\n",
    "        y = df_data[i][j]['demand'].values\n",
    "        df_train_X[i][j], X_temp, df_train_y[i][j], y_temp = train_test_split(X, y, test_size = 0.3, random_state=42)\n",
    "        df_valid_X[i][j], df_test_X[i][j], df_valid_y[i][j], df_test_y[i][j] = train_test_split(X_temp, y_temp, test_size = 1/3, random_state=42)\n",
    "        df_train_X[i][j] = feature_scaler.fit_transform(df_train_X[i][j])\n",
    "        df_valid_X[i][j] = feature_scaler.transform(df_valid_X[i][j])\n",
    "        df_test_X[i][j] = feature_scaler.transform(df_test_X[i][j])\n",
    "        joblib.dump(feature_scaler, f'scalers/feature_scaler_res_{i + 6}_bucket_{timebuckets[j]}.pkl')\n",
    "        df_train_y[i][j] = target_scaler.fit_transform(df_train_y[i][j].reshape(-1, 1)).flatten()\n",
    "        df_valid_y[i][j] = target_scaler.transform(df_valid_y[i][j].reshape(-1, 1)).flatten()\n",
    "        df_test_y[i][j] = target_scaler.transform(df_test_y[i][j].reshape(-1, 1)).flatten()\n",
    "        joblib.dump(target_scaler, f'scalers/target_scaler_res_{i + 6}_bucket_{timebuckets[j]}.pkl')\n",
    "        df_train_X[i][j] = torch.tensor(df_train_X[i][j], dtype = torch.float32)\n",
    "        df_train_y[i][j] = torch.tensor(df_train_y[i][j], dtype = torch.float32).view(-1, 1)\n",
    "        df_valid_X[i][j] = torch.tensor(df_valid_X[i][j], dtype = torch.float32)\n",
    "        df_valid_y[i][j] = torch.tensor(df_valid_y[i][j], dtype=torch.float32).view(-1, 1)\n",
    "        df_test_X[i][j] = torch.tensor(df_test_X[i][j], dtype=torch.float32)\n",
    "        df_test_y[i][j] = torch.tensor(df_test_y[i][j], dtype=torch.float32).view(-1, 1)\n",
    "        torch.save(TensorDataset(df_train_X[i][j], df_train_y[i][j]), f'datasets/train_dataset_res_{i + 6}_bucket_{timebuckets[j]}.pt')\n",
    "        torch.save(TensorDataset(df_valid_X[i][j], df_valid_y[i][j]), f'datasets/valid_dataset_res_{i + 6}_bucket_{timebuckets[j]}.pt')\n",
    "        torch.save(TensorDataset(df_test_X[i][j], df_test_y[i][j]), f'datasets/test_dataset_res_{i + 6}_bucket_{timebuckets[j]}.pt')\n",
    "del(df_merged, df_data, df_train_X, df_valid_X, df_test_X, df_train_y, df_valid_y, df_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6a0fe0-1dd4-49c2-a24c-f9632c64d72e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
